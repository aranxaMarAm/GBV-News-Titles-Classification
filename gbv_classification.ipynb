{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a GBV classifier from a previously labeled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used libraries\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                link state  \\\n",
      "0  https://web.archive.org/web/20200901174745/htt...  CHIH   \n",
      "1  https://web.archive.org/web/20200721132743/htt...  CHIH   \n",
      "2    http://laopcion.com.mx/noticia/98812?archivo=si  CHIH   \n",
      "3  https://web.archive.org/web/20200901181614/htt...  CHIH   \n",
      "4  https://web.archive.org/web/20200901184921/htt...  CHIH   \n",
      "\n",
      "                                               title      frame  \n",
      "0  Imparte fiscalía pláticas preventivas a emplea...   Temático  \n",
      "1  La atropella su pareja y la deja lesionada al ...  Episódico  \n",
      "2  Detienen a chofer de camión urbano por hostiga...  Episódico  \n",
      "3  Inaugura Duarte Centro de Salud y Albergue Cie...   Temático  \n",
      "4  Presentan la conferencia La grandeza de ser mu...   Temático  \n"
     ]
    }
   ],
   "source": [
    "# Load the labeled data\n",
    "file_path = 'gbv_df.xlsx'\n",
    "gbv_df = pd.read_excel(file_path)\n",
    "\n",
    "# Inspect the data\n",
    "print(gbv_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0  Imparte fiscalía pláticas preventivas a emplea...   \n",
      "1  La atropella su pareja y la deja lesionada al ...   \n",
      "2  Detienen a chofer de camión urbano por hostiga...   \n",
      "3  Inaugura Duarte Centro de Salud y Albergue Cie...   \n",
      "4  Presentan la conferencia La grandeza de ser mu...   \n",
      "\n",
      "                                       cleaned_title  \n",
      "0  imparte fiscala plticas preventivas a empleado...  \n",
      "1  la atropella su pareja y la deja lesionada al ...  \n",
      "2  detienen a chofer de camin urbano por hostigam...  \n",
      "3  inaugura duarte centro de salud y albergue cie...  \n",
      "4  presentan la conferencia la grandeza de ser mu...  \n"
     ]
    }
   ],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove non-alphabetical characters\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to the 'title' column\n",
    "gbv_df['cleaned_title'] = gbv_df['title'].apply(preprocess_text)\n",
    "\n",
    "# Display the cleaned text\n",
    "print(gbv_df[['title', 'cleaned_title']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean frame labels and remove accents\n",
    "def clean_labels(label):\n",
    "    # Normalize the text to decompose accents\n",
    "    label = unicodedata.normalize('NFD', label)\n",
    "    # Remove diacritics (accents) by filtering characters\n",
    "    label = ''.join(char for char in label if unicodedata.category(char) != 'Mn')\n",
    "    # Convert to lowercase and strip whitespace\n",
    "    label = label.lower().strip()\n",
    "    return label\n",
    "\n",
    "# Apply the cleaning function to the 'frame' column\n",
    "gbv_df['frames'] = gbv_df['frame'].apply(clean_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['episodico' 'tematico']\n"
     ]
    }
   ],
   "source": [
    "# Define features (X) and labels (y)\n",
    "X = gbv_df['cleaned_title']\n",
    "y = gbv_df['frames']  \n",
    "\n",
    "# Encode labels if necessary\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)  # Converts to numerical values\n",
    "print(label_encoder.classes_)  # Check the mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 777, Test samples: 195\n"
     ]
    }
   ],
   "source": [
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}, Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape (training): (777, 1775)\n"
     ]
    }
   ],
   "source": [
    "# Use TF-IDF for feature extraction\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Adjust max_features as needed\n",
    "\n",
    "# Fit the vectorizer on training data and transform both training and test data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"TF-IDF matrix shape (training): {X_train_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9128205128205128\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   episodico       0.91      0.93      0.92       103\n",
      "    tematico       0.92      0.89      0.91        92\n",
      "\n",
      "    accuracy                           0.91       195\n",
      "   macro avg       0.91      0.91      0.91       195\n",
      "weighted avg       0.91      0.91      0.91       195\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train a logistic regression model\n",
    "logreg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "logreg.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = logreg.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer.pkl']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained model and vectorizer\n",
    "joblib.dump(logreg, 'logreg_model.pkl')\n",
    "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping for more news titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://8columnas.com.mx/?s=mujer\n",
      "Fetching: https://8columnas.com.mx/page/2/?s=mujer\n",
      "Fetching: https://8columnas.com.mx/page/3/?s=mujer\n",
      "Fetching: https://8columnas.com.mx/page/4/?s=mujer\n",
      "Fetching: https://8columnas.com.mx/page/5/?s=mujer\n",
      "Fetching: https://8columnas.com.mx/page/6/?s=mujer\n",
      "Fetching: https://8columnas.com.mx/page/7/?s=mujer\n",
      "Fetching: https://8columnas.com.mx/page/8/?s=mujer\n",
      "Fetching: https://8columnas.com.mx/page/9/?s=mujer\n",
      "Fetching: https://8columnas.com.mx/page/10/?s=mujer\n",
      "Scraping completed and data saved to '8c_titles_mujer.csv'.\n"
     ]
    }
   ],
   "source": [
    "## Successfull scraping\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_titles_and_links(base_url, max_pages=10):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\"\n",
    "    }\n",
    "    results = []\n",
    "\n",
    "    # Iterate through pages\n",
    "    for page_number in range(1, max_pages + 1):\n",
    "        # Construct URL based on page number\n",
    "        if page_number == 1:\n",
    "            url = base_url\n",
    "        else:\n",
    "            url = f\"https://8columnas.com.mx/page/{page_number}/?s=mujer\"  # Adjust for pagination\n",
    "        \n",
    "        print(f\"Fetching: {url}\")\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        # Check for response errors\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch page: {url}, Status Code: {response.status_code}\")\n",
    "            continue\n",
    "        \n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        articles = soup.find_all(\"h3\", class_=\"entry-title\")\n",
    "        \n",
    "        # Extract titles and links\n",
    "        for article in articles:\n",
    "            title_tag = article.find(\"a\", href=True)\n",
    "            if title_tag:\n",
    "                title = title_tag.get_text(strip=True)\n",
    "                link = title_tag[\"href\"]\n",
    "                results.append({\"link\": link, \"title\": title, \"state\": \"edo\"})\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Save data to CSV\n",
    "def save_to_csv(data, filename=\"8c_titles_mujer.csv\"):\n",
    "    import csv\n",
    "    with open(filename, mode=\"w\", encoding=\"utf-8\", newline=\"\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"link\", \"title\", \"state\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "# Main Function\n",
    "if __name__ == \"__main__\":\n",
    "    # Base URL includes the keyword\n",
    "    base_url = \"https://8columnas.com.mx/?s=mujer\"\n",
    "    articles = extract_titles_and_links(base_url, max_pages=10)  # Adjust number of pages as needed\n",
    "    save_to_csv(articles)\n",
    "    print(\"Scraping completed and data saved to '8c_titles_mujer.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching articles for keyword: mujer\n",
      "Fetching: https://8columnas.com.mx?s=mujer\n",
      "Fetching: https://8columnas.com.mx/page/2/?s=mujer\n",
      "Fetching: https://8columnas.com.mx/page/3/?s=mujer\n",
      "Fetching: https://8columnas.com.mx/page/4/?s=mujer\n",
      "Fetching: https://8columnas.com.mx/page/5/?s=mujer\n",
      "Fetching: https://8columnas.com.mx/page/6/?s=mujer\n",
      "Fetching: https://8columnas.com.mx/page/7/?s=mujer\n",
      "Fetching: https://8columnas.com.mx/page/8/?s=mujer\n",
      "Fetching: https://8columnas.com.mx/page/9/?s=mujer\n",
      "Fetching: https://8columnas.com.mx/page/10/?s=mujer\n",
      "Fetching articles for keyword: niña\n",
      "Fetching: https://8columnas.com.mx?s=niña\n",
      "Fetching: https://8columnas.com.mx/page/2/?s=niña\n",
      "Fetching: https://8columnas.com.mx/page/3/?s=niña\n",
      "Fetching: https://8columnas.com.mx/page/4/?s=niña\n",
      "Fetching: https://8columnas.com.mx/page/5/?s=niña\n",
      "Fetching: https://8columnas.com.mx/page/6/?s=niña\n",
      "Fetching: https://8columnas.com.mx/page/7/?s=niña\n",
      "Fetching: https://8columnas.com.mx/page/8/?s=niña\n",
      "Fetching: https://8columnas.com.mx/page/9/?s=niña\n",
      "Fetching: https://8columnas.com.mx/page/10/?s=niña\n",
      "Fetching articles for keyword: feminicidio\n",
      "Fetching: https://8columnas.com.mx?s=feminicidio\n",
      "Fetching: https://8columnas.com.mx/page/2/?s=feminicidio\n",
      "Fetching: https://8columnas.com.mx/page/3/?s=feminicidio\n",
      "Fetching: https://8columnas.com.mx/page/4/?s=feminicidio\n",
      "Fetching: https://8columnas.com.mx/page/5/?s=feminicidio\n",
      "Fetching: https://8columnas.com.mx/page/6/?s=feminicidio\n",
      "Fetching: https://8columnas.com.mx/page/7/?s=feminicidio\n",
      "Fetching: https://8columnas.com.mx/page/8/?s=feminicidio\n",
      "Fetching: https://8columnas.com.mx/page/9/?s=feminicidio\n",
      "Fetching: https://8columnas.com.mx/page/10/?s=feminicidio\n",
      "Fetching articles for keyword: violencia+de+género\n",
      "Fetching: https://8columnas.com.mx?s=violencia+de+género\n",
      "Fetching: https://8columnas.com.mx/page/2/?s=violencia+de+género\n",
      "Fetching: https://8columnas.com.mx/page/3/?s=violencia+de+género\n",
      "Fetching: https://8columnas.com.mx/page/4/?s=violencia+de+género\n",
      "Fetching: https://8columnas.com.mx/page/5/?s=violencia+de+género\n",
      "Fetching: https://8columnas.com.mx/page/6/?s=violencia+de+género\n",
      "Fetching: https://8columnas.com.mx/page/7/?s=violencia+de+género\n",
      "Fetching: https://8columnas.com.mx/page/8/?s=violencia+de+género\n",
      "Fetching: https://8columnas.com.mx/page/9/?s=violencia+de+género\n",
      "Fetching: https://8columnas.com.mx/page/10/?s=violencia+de+género\n",
      "Scraping completed and data saved to '8c_titles.csv'.\n"
     ]
    }
   ],
   "source": [
    "## Trying with more keywords\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "def extract_titles_and_links(base_url, keyword, max_pages=10):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\"\n",
    "    }\n",
    "    results = []\n",
    "\n",
    "    # Iterate through pages\n",
    "    for page_number in range(1, max_pages + 1):\n",
    "        # Construct URL for each page\n",
    "        if page_number == 1:\n",
    "            url = f\"{base_url}?s={keyword}\"\n",
    "        else:\n",
    "            url = f\"{base_url}/page/{page_number}/?s={keyword}\"\n",
    "        \n",
    "        print(f\"Fetching: {url}\")\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        # Check for response errors\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch page: {url}, Status Code: {response.status_code}\")\n",
    "            continue\n",
    "        \n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        articles = soup.find_all(\"h3\", class_=\"entry-title\")\n",
    "        \n",
    "        # Extract titles and links\n",
    "        for article in articles:\n",
    "            title_tag = article.find(\"a\", href=True)\n",
    "            if title_tag:\n",
    "                title = title_tag.get_text(strip=True)\n",
    "                link = title_tag[\"href\"]\n",
    "                results.append({\"link\": link, \"title\": title, \"keyword\": keyword})\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Save data to CSV\n",
    "def save_to_csv(data, filename=\"8c_titles.csv\"):\n",
    "    with open(filename, mode=\"w\", encoding=\"utf-8\", newline=\"\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"link\", \"title\", \"keyword\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "# Main Function\n",
    "if __name__ == \"__main__\":\n",
    "    # Base URL and keywords\n",
    "    base_url = \"https://8columnas.com.mx\"\n",
    "    keywords = [\"mujer\", \"niña\", \"feminicidio\", \"violencia+de+género\"] \n",
    "    all_results = []\n",
    "\n",
    "    for keyword in keywords:\n",
    "        print(f\"Fetching articles for keyword: {keyword}\")\n",
    "        results = extract_titles_and_links(base_url, keyword, max_pages=10)\n",
    "        all_results.extend(results)  # Combine results for all keywords\n",
    "    \n",
    "    # Save all results into a single CSV\n",
    "    save_to_csv(all_results, filename=\"8c_titles.csv\")\n",
    "    print(\"Scraping completed and data saved to '8c_titles.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching articles for keyword: mujer\n",
      "Fetching: https://8columnas.com.mx?s=mujer\n",
      "Fetching: https://8columnas.com.mx/page/2/?s=mujer\n",
      "Fetching: https://8columnas.com.mx/page/3/?s=mujer\n",
      "Fetching: https://8columnas.com.mx/page/4/?s=mujer\n",
      "Fetching: https://8columnas.com.mx/page/5/?s=mujer\n",
      "Fetching: https://8columnas.com.mx/page/6/?s=mujer\n",
      "Fetching: https://8columnas.com.mx/page/7/?s=mujer\n",
      "Fetching: https://8columnas.com.mx/page/8/?s=mujer\n",
      "Fetching: https://8columnas.com.mx/page/9/?s=mujer\n",
      "Fetching: https://8columnas.com.mx/page/10/?s=mujer\n",
      "Fetching articles for keyword: niña\n",
      "Fetching: https://8columnas.com.mx?s=niña\n",
      "Fetching: https://8columnas.com.mx/page/2/?s=niña\n",
      "Fetching: https://8columnas.com.mx/page/3/?s=niña\n",
      "Fetching: https://8columnas.com.mx/page/4/?s=niña\n",
      "Fetching: https://8columnas.com.mx/page/5/?s=niña\n",
      "Fetching: https://8columnas.com.mx/page/6/?s=niña\n",
      "Fetching: https://8columnas.com.mx/page/7/?s=niña\n",
      "Fetching: https://8columnas.com.mx/page/8/?s=niña\n",
      "Fetching: https://8columnas.com.mx/page/9/?s=niña\n",
      "Fetching: https://8columnas.com.mx/page/10/?s=niña\n",
      "Fetching articles for keyword: feminicidio\n",
      "Fetching: https://8columnas.com.mx?s=feminicidio\n",
      "Fetching: https://8columnas.com.mx/page/2/?s=feminicidio\n",
      "Fetching: https://8columnas.com.mx/page/3/?s=feminicidio\n",
      "Fetching: https://8columnas.com.mx/page/4/?s=feminicidio\n",
      "Fetching: https://8columnas.com.mx/page/5/?s=feminicidio\n",
      "Fetching: https://8columnas.com.mx/page/6/?s=feminicidio\n",
      "Fetching: https://8columnas.com.mx/page/7/?s=feminicidio\n",
      "Fetching: https://8columnas.com.mx/page/8/?s=feminicidio\n",
      "Fetching: https://8columnas.com.mx/page/9/?s=feminicidio\n",
      "Fetching: https://8columnas.com.mx/page/10/?s=feminicidio\n",
      "Fetching articles for keyword: violencia+de+género\n",
      "Fetching: https://8columnas.com.mx?s=violencia+de+género\n",
      "Fetching: https://8columnas.com.mx/page/2/?s=violencia+de+género\n",
      "Fetching: https://8columnas.com.mx/page/3/?s=violencia+de+género\n",
      "Fetching: https://8columnas.com.mx/page/4/?s=violencia+de+género\n",
      "Fetching: https://8columnas.com.mx/page/5/?s=violencia+de+género\n",
      "Fetching: https://8columnas.com.mx/page/6/?s=violencia+de+género\n",
      "Fetching: https://8columnas.com.mx/page/7/?s=violencia+de+género\n",
      "Fetching: https://8columnas.com.mx/page/8/?s=violencia+de+género\n",
      "Fetching: https://8columnas.com.mx/page/9/?s=violencia+de+género\n",
      "Fetching: https://8columnas.com.mx/page/10/?s=violencia+de+género\n",
      "Scraping completed, filtered data saved to '8c_titles.csv'.\n"
     ]
    }
   ],
   "source": [
    "### Removing \"Top read news\" that also got fetched articles\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "def extract_titles_and_links(base_url, keyword, max_pages=10):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\"\n",
    "    }\n",
    "    results = []\n",
    "\n",
    "    # Iterate through pages\n",
    "    for page_number in range(1, max_pages + 1):\n",
    "        # Construct URL for each page\n",
    "        if page_number == 1:\n",
    "            url = f\"{base_url}?s={keyword}\"\n",
    "        else:\n",
    "            url = f\"{base_url}/page/{page_number}/?s={keyword}\"\n",
    "        \n",
    "        print(f\"Fetching: {url}\")\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        # Check for response errors\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch page: {url}, Status Code: {response.status_code}\")\n",
    "            continue\n",
    "        \n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        articles = soup.find_all(\"h3\", class_=\"entry-title\")\n",
    "        \n",
    "        # Extract titles and links\n",
    "        for article in articles:\n",
    "            title_tag = article.find(\"a\", href=True)\n",
    "            if title_tag:\n",
    "                title = title_tag.get_text(strip=True)\n",
    "                link = title_tag[\"href\"]\n",
    "                results.append({\"link\": link, \"title\": title, \"keyword\": keyword})\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Save data to CSV\n",
    "def save_to_csv(data, filename=\"8c_titles.csv\"):\n",
    "    # To remove titles that appear on the side as \"most popular\"\n",
    "    excluded_titles = [\n",
    "        \"Rinde protesta como presidente de Almoloya de Juárez, Adolfo Solís\",\n",
    "        \"Se mantiene en Edoméx tendencia a la baja en homicidio doloso\",\n",
    "        \"Petro venderá la residencia del embajador en México para mejorar servicios consulares\",\n",
    "        \"TikTok enfrenta investigación de la UE por presunta interferencia en elecciones presidenciales de Rumanía\"\n",
    "    ]\n",
    "\n",
    "    # Filter out excluded titles\n",
    "    filtered_data = [row for row in data if row[\"title\"] not in excluded_titles]\n",
    "\n",
    "    # Write the filtered data to CSV\n",
    "    with open(filename, mode=\"w\", encoding=\"utf-8\", newline=\"\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"link\", \"title\", \"keyword\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(filtered_data)\n",
    "\n",
    "# Main Function\n",
    "if __name__ == \"__main__\":\n",
    "    # Base URL and keywords\n",
    "    base_url = \"https://8columnas.com.mx\"\n",
    "    keywords = [\"mujer\", \"niña\", \"feminicidio\", \"violencia+de+género\"]\n",
    "    all_results = []\n",
    "\n",
    "    for keyword in keywords:\n",
    "        print(f\"Fetching articles for keyword: {keyword}\")\n",
    "        results = extract_titles_and_links(base_url, keyword, max_pages=10)\n",
    "        all_results.extend(results)  # Combine results for all keywords\n",
    "    \n",
    "    # Save filtered results to CSV\n",
    "    save_to_csv(all_results, filename=\"8c_titles.csv\")\n",
    "    print(\"Scraping completed, filtered data saved to '8c_titles.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 'state' column with value 'edo' to the CSV.\n"
     ]
    }
   ],
   "source": [
    "# Read the existing CSV file\n",
    "df = pd.read_csv('8c_titles.csv')\n",
    "\n",
    "# Add the new 'observations' column with the value 'edo' for all rows\n",
    "df['state'] = 'edo'\n",
    "\n",
    "# Save the updated DataFrame to the same CSV file\n",
    "df.to_csv('8c_titles.csv', index=False)\n",
    "\n",
    "print(\"Added 'state' column with value 'edo' to the CSV.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# La Opción de Chihuahua"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key word: \"mujer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch titles\n",
    "def fetch_titles_with_selenium(base_url, keyword, max_pages=10):\n",
    "    # Set up WebDriver\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")  # Run in headless mode\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    \n",
    "    # Construct the initial URL with the keyword\n",
    "    url = f\"{base_url}?q={keyword}&cx=&ie=UTF-8\"\n",
    "    print(f\"Fetching: {url}\")\n",
    "    driver.get(url)\n",
    "    \n",
    "    titles = []\n",
    "    current_page = 1\n",
    "\n",
    "    while current_page <= max_pages:\n",
    "        try:\n",
    "            # Wait for the page navigation and content to load\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a.gs-title\"))\n",
    "            )\n",
    "            time.sleep(2)  # Additional wait for safety\n",
    "            \n",
    "            # Find all title elements on the current page\n",
    "            articles = driver.find_elements(By.CSS_SELECTOR, \"a.gs-title\")\n",
    "            for article in articles:\n",
    "                title = article.text\n",
    "                link = article.get_attribute(\"href\")\n",
    "                if title and link:  # Ensure both title and link are valid\n",
    "                    titles.append({\"link\": link, \"title\": title, \"keyword\": keyword, \"state\": \"chih\"})\n",
    "            \n",
    "            print(f\"Fetched titles from page {current_page}.\")\n",
    "            \n",
    "            # Try to find the next page button dynamically\n",
    "            try:\n",
    "                next_button = driver.find_element(By.CSS_SELECTOR, f\"div.gsc-cursor-page[aria-label='Página {current_page + 1}']\")\n",
    "                ActionChains(driver).move_to_element(next_button).click().perform()\n",
    "                current_page += 1\n",
    "                time.sleep(2)  # Allow time for the next page to load\n",
    "            except Exception:\n",
    "                print(f\"No more pages available after page {current_page}.\")\n",
    "                break\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error during scraping on page {current_page}: {e}\")\n",
    "            break\n",
    "    \n",
    "    driver.quit()\n",
    "    return titles\n",
    "\n",
    "# Save data to CSV\n",
    "def save_to_csv(data, filename=\"titles.csv\"):\n",
    "    with open(filename, mode=\"w\", encoding=\"utf-8\", newline=\"\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"link\", \"title\", \"keyword\", \"state\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "# Successfully save the 100 notes for the keyword \"mujer\"\n",
    "if __name__ == \"__main__\":\n",
    "    base_url = \"https://laopcion.com.mx/buscar.html\"\n",
    "    keyword = \"mujer\"  # Replace with the desired keyword\n",
    "    max_pages = 10\n",
    "    titles = fetch_titles_with_selenium(base_url, keyword, max_pages)\n",
    "    \n",
    "    if titles:\n",
    "        print(f\"Fetched {len(titles)} titles.\")\n",
    "        save_to_csv(titles, f\"{keyword}_titles.csv\")\n",
    "        print(f\"Data saved to '{keyword}_titles.csv'.\")\n",
    "    else:\n",
    "        print(\"No titles were fetched.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key words: \"niña\" and \"Violencia de género\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import csv\n",
    "from fake_useragent import UserAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching titles for keyword: niña\n",
      "Fetching: https://laopcion.com.mx/buscar.html?q=niña&cx=&ie=UTF-8\n",
      "Fetched titles from page 1.\n",
      "Fetched titles from page 2.\n",
      "Fetched titles from page 3.\n",
      "Fetched titles from page 4.\n",
      "Fetched titles from page 5.\n",
      "Fetched titles from page 6.\n",
      "Fetched titles from page 7.\n",
      "Fetched titles from page 8.\n",
      "Fetched titles from page 9.\n",
      "Fetched titles from page 10.\n",
      "No more pages available after page 10.\n",
      "Fetched 100 titles for keyword 'niña'.\n",
      "Fetching titles for keyword: violencia+de+género\n",
      "Fetching: https://laopcion.com.mx/buscar.html?q=violencia+de+género&cx=&ie=UTF-8\n",
      "Fetched titles from page 1.\n",
      "Fetched titles from page 2.\n",
      "Fetched titles from page 3.\n",
      "Fetched titles from page 4.\n",
      "Fetched titles from page 5.\n",
      "Fetched titles from page 6.\n",
      "Fetched titles from page 7.\n",
      "Fetched titles from page 8.\n",
      "Fetched titles from page 9.\n",
      "Fetched titles from page 10.\n",
      "No more pages available after page 10.\n",
      "Fetched 100 titles for keyword 'violencia+de+género'.\n",
      "Data saved to 'niña_vdg_titles.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Function to fetch titles\n",
    "def fetch_titles_with_selenium(base_url, keyword, max_pages=10):\n",
    "    # Set up WebDriver\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")  # Run in headless mode\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    \n",
    "    # Construct the initial URL with the keyword\n",
    "    url = f\"{base_url}?q={keyword}&cx=&ie=UTF-8\"\n",
    "    print(f\"Fetching: {url}\")\n",
    "    driver.get(url)\n",
    "    \n",
    "    titles = []\n",
    "    current_page = 1\n",
    "\n",
    "    while current_page <= max_pages:\n",
    "        try:\n",
    "            # Wait for the page navigation and content to load\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a.gs-title\"))\n",
    "            )\n",
    "            time.sleep(2)  # Additional wait for safety\n",
    "            \n",
    "            # Find all title elements on the current page\n",
    "            articles = driver.find_elements(By.CSS_SELECTOR, \"a.gs-title\")\n",
    "            for article in articles:\n",
    "                title = article.text\n",
    "                link = article.get_attribute(\"href\")\n",
    "                if title and link:  # Ensure both title and link are valid\n",
    "                    titles.append({\"link\": link, \"title\": title, \"keyword\": keyword, \"state\": \"chih\"})\n",
    "            \n",
    "            print(f\"Fetched titles from page {current_page}.\")\n",
    "            \n",
    "            # Try to find the next page button dynamically\n",
    "            try:\n",
    "                next_button = driver.find_element(By.CSS_SELECTOR, f\"div.gsc-cursor-page[aria-label='Página {current_page + 1}']\")\n",
    "                ActionChains(driver).move_to_element(next_button).click().perform()\n",
    "                current_page += 1\n",
    "                time.sleep(2)  # Allow time for the next page to load\n",
    "            except Exception:\n",
    "                print(f\"No more pages available after page {current_page}.\")\n",
    "                break\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error during scraping on page {current_page}: {e}\")\n",
    "            break\n",
    "    \n",
    "    driver.quit()\n",
    "    return titles\n",
    "\n",
    "# Save data to CSV\n",
    "def save_to_csv(data, filename=\"titles.csv\"):\n",
    "    with open(filename, mode=\"w\", encoding=\"utf-8\", newline=\"\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"link\", \"title\", \"keyword\", \"state\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "# Main function to fetch for multiple keywords\n",
    "if __name__ == \"__main__\":\n",
    "    base_url = \"https://laopcion.com.mx/buscar.html\"\n",
    "    keywords = [\"niña\", \"violencia+de+género\"]  # The desired keywords\n",
    "    max_pages = 10\n",
    "    \n",
    "    all_titles = []\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        print(f\"Fetching titles for keyword: {keyword}\")\n",
    "        titles = fetch_titles_with_selenium(base_url, keyword, max_pages)\n",
    "        \n",
    "        if titles:\n",
    "            print(f\"Fetched {len(titles)} titles for keyword '{keyword}'.\")\n",
    "            all_titles.extend(titles)\n",
    "        else:\n",
    "            print(f\"No titles found for keyword '{keyword}'.\")\n",
    "    \n",
    "    # Save all fetched titles to CSV\n",
    "    if all_titles:\n",
    "        save_to_csv(all_titles, \"niña_vdg_titles.csv\")\n",
    "        print(\"Data saved to 'niña_vdg_titles.csv'.\")\n",
    "    else:\n",
    "        print(\"No data to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword: \"feminicidio\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://laopcion.com.mx/buscar.html?q=feminicidio&cx=&ie=UTF-8\n",
      "Fetched titles from page 1.\n",
      "Fetched titles from page 2.\n",
      "Fetched titles from page 3.\n",
      "Fetched titles from page 4.\n",
      "Fetched titles from page 5.\n",
      "Fetched titles from page 6.\n",
      "Fetched titles from page 7.\n",
      "Fetched titles from page 8.\n",
      "Fetched titles from page 9.\n",
      "Fetched titles from page 10.\n",
      "No more pages available after page 10.\n",
      "Fetched 100 titles.\n",
      "Data saved to 'feminicidio_titles.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Function to fetch titles\n",
    "def fetch_titles_with_selenium(base_url, keyword, max_pages=10):\n",
    "    # Set up WebDriver\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")  # Run in headless mode\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    \n",
    "    # Construct the initial URL with the keyword\n",
    "    url = f\"{base_url}?q={keyword}&cx=&ie=UTF-8\"\n",
    "    print(f\"Fetching: {url}\")\n",
    "    driver.get(url)\n",
    "    \n",
    "    titles = []\n",
    "    current_page = 1\n",
    "\n",
    "    while current_page <= max_pages:\n",
    "        try:\n",
    "            # Wait for the page navigation and content to load\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a.gs-title\"))\n",
    "            )\n",
    "            time.sleep(2)  # Additional wait for safety\n",
    "            \n",
    "            # Find all title elements on the current page\n",
    "            articles = driver.find_elements(By.CSS_SELECTOR, \"a.gs-title\")\n",
    "            for article in articles:\n",
    "                title = article.text\n",
    "                link = article.get_attribute(\"href\")\n",
    "                if title and link:  # Ensure both title and link are valid\n",
    "                    titles.append({\"link\": link, \"title\": title, \"keyword\": keyword, \"state\": \"chih\"})\n",
    "            \n",
    "            print(f\"Fetched titles from page {current_page}.\")\n",
    "            \n",
    "            # Try to find the next page button dynamically\n",
    "            try:\n",
    "                next_button = driver.find_element(By.CSS_SELECTOR, f\"div.gsc-cursor-page[aria-label='Página {current_page + 1}']\")\n",
    "                ActionChains(driver).move_to_element(next_button).click().perform()\n",
    "                current_page += 1\n",
    "                time.sleep(2)  # Allow time for the next page to load\n",
    "            except Exception:\n",
    "                print(f\"No more pages available after page {current_page}.\")\n",
    "                break\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error during scraping on page {current_page}: {e}\")\n",
    "            break\n",
    "    \n",
    "    driver.quit()\n",
    "    return titles\n",
    "\n",
    "# Save data to CSV\n",
    "def save_to_csv(data, filename=\"titles.csv\"):\n",
    "    with open(filename, mode=\"w\", encoding=\"utf-8\", newline=\"\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"link\", \"title\", \"keyword\", \"state\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "# Main function to scrape \"feminicidio\" titles\n",
    "if __name__ == \"__main__\":\n",
    "    base_url = \"https://laopcion.com.mx/buscar.html\"\n",
    "    keyword = \"feminicidio\"  # Fetch for \"feminicidio\"\n",
    "    max_pages = 10\n",
    "    titles = fetch_titles_with_selenium(base_url, keyword, max_pages)\n",
    "    \n",
    "    if titles:\n",
    "        print(f\"Fetched {len(titles)} titles.\")\n",
    "        save_to_csv(titles, f\"{keyword}_titles.csv\")\n",
    "        print(f\"Data saved to '{keyword}_titles.csv'.\")\n",
    "    else:\n",
    "        print(\"No titles were fetched.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging the new titles from \"8 Columnas\" and \"La Opción de Chihuahua\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All titles have been combined into 'loc_titles.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of CSV files to combine\n",
    "csv_files = [\"feminicidio_titles.csv\", \"mujer_titles.csv\", \"niña_vdg_titles.csv\"]\n",
    "\n",
    "# List to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Read and append each CSV file into the list of DataFrames\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "combined_df.to_csv(\"loc_titles.csv\", index=False)\n",
    "\n",
    "print(\"All titles have been combined into 'loc_titles.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
